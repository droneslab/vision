<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="VISION"/>
  <meta property="og:description" content="Visual Inspection System with Intelligent Observation and Navigation"/>
  <meta property="og:url" content="https://droneslab.github.io/VISION/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/VISION-Logo.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="VISION">
  <meta name="twitter:description" content="Visual Inspection System with Intelligent Observation and Navigation">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/VISION-Logo">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Featureness">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VISION : Visual Inspection System with Intelligent Observation and Navigation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span style="color: rgb(192, 0, 0)">VISION</span>: Visual Inspection System with Intelligent Observation and Navigation</h1>
            <!-- <h1 style="font-size:1.5rem">European Conference on Computer Vision (ECCV) 2024</h1> -->
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yash.turkar.in/" target="_blank">Yash Turkar</a><sup>*</sup>,</span>
                <span class="author-block">
                <a href="https://yashomdighe.com/" target="_blank">Yashom Dighe</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://cse.buffalo.edu/faculty/kdantu/" target="_blank">Karthik Dantu</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <sup>*</sup> Equal contribution
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Center for Embodied Autonomy and Robotics (CEAR) <br> University at Buffalo</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <a href="https://www.buffalo.edu/">
                      <img style="width:30%; padding-right: 15px;" src="static/images/ub_logo.png">
                    </a>
<!--                     <a href="https://eccv2024.ecva.net/">
                      <img style="width:30%; height: 30%; padding-right: 15px;" src="static/images/eccv_logo.png">
                    </a> -->
                    <a href="http://drones.cse.buffalo.edu/">
                      <img style="width:30%; height:auto; padding-bottom: 25px;" src="static/images/drones_logo.png">
                    </a>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="http://arxiv.org/abs/2509.21370" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Arxiv</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary (Coming Soon)</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/droneslab/vision/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>

                <span class="link-block">
                    <a href="https://droneslab.github.io/INSPECTOR"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>INSPECTOR</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->


              <div class="content has-text-justified">
                <img style="width:100%;" src="static/images/VISION-Splash3.pdf">
                <p>
                  Schematic of the Erie Canal culvert inspection setup. The cross-section of the canal and culvert is drawn to scale, derived from a
                  3D reconstruction of the site, illustrating the buried drainage conduit, a.k.a. the culvert, beneath the canal embankment. Culverts provide
                  critical drainage but their confined geometry and location make inspection difficult. A Boston Dynamics Spot quadruped, outfitted with a
                  custom inspection payload (pan–tilt gimbal, inspection camera with co-located light, VLM proposal camera, and auxiliary LED flood), is
                  deployed at the culvert entrance and traverses the interior. Insets show the robot at the portal and during inspection runs inside the conduit.
                  Only the robot payload illustration is not to scale; all other canal and culvert geometry is based on the site reconstruction. The system
                  autonomously navigates through the 66 m long, 1.2 m diameter culvert, capturing targeted images for structural condition assessment.
                </p>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- <section class="section hero "></section>
  <div class="container is-max-desktop">
    <iframe height="540" width="960" src="https://www.youtube.com/embed/nX56DGGrWxk?si=ii3xg8ZpZLpxs5Xv" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
  </div>
</section> -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Culverts on canals such as Erie Canal built originally in 1825 require frequent inspections to ensure safe operation. Human inspection of culverts is challenging due to age, geometry, poor illumination, weather and lack of easy access. We introduce VISION, an end-to-end, language-in-the-loop autonomy system that couples a web-scale vision–language model (VLM) with constrained viewpoint planning for autonomous inspection of culverts. Brief prompts to the VLM solicit open-vocabulary ROI proposals with rationales and confidences, stereo depth is fused to recover scale, and a planner—aware of culvert constraints commands repositioning moves to capture targeted close-ups. Deployed on a quadruped in Culvert under the Erie canal, VISION closes the see→decide→move→re-image loop on-board and produces high-resolution images for detailed reporting without domain-specific fine-tuning. In an external evaluation by New York Canal Corporation personnel, initial ROI proposals achieved 61.4% agreement with subject-matter experts, and final post-re-imaging assessments reached 80%, indicating that VISION converts tentative hypotheses into grounded, expert-aligned findings.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video Presentation</h2>
        <div class="publication-video">
          <iframe height="540" width="960" src="https://www.youtube.com/embed/nX56DGGrWxk?si=ii3xg8ZpZLpxs5Xv" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>
</section>

<!--Youtube video -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results Visualized</h2>

        <!-- Wrap in a Bulma ratio box -->
        <figure class="image is-16by9" style="max-width: 900px; margin: 0 auto;">
          <iframe
            id="rrd-frame"
            class="has-ratio"
            src="https://yashturkar.github.io/vision-rrd/"
            style="border:0;"  
            allow="fullscreen"
            loading="lazy"
          ></iframe>
        </figure>

        <p class="is-size-7">View fullscreen for best experience</p>
        <div class="buttons is-centered">
          <button class="button is-small"
                  onclick="document.getElementById('rrd-frame').requestFullscreen()">
            Fullscreen
          </button>
          <a class="button is-light is-small" target="_blank" rel="noopener"
             href="https://yashturkar.github.io/vision-rrd/">
            Open in new tab
          </a>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!-- Method -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column"><h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            The training of VISION is a three-step process. First, we train a network with a general understanding of interestingness (i.e., feature point detection) where we make use of SiLK in this work (top left). Next, we convert this model to a Bayesian Neural Network (BNN) and train again using the addition of probabilistic losses (e.g., KL Divergence, top middle). Finally, we train a specialized uncertainty head using feature variance computed by Monte Carlo supervision from the BNN (top right). The VISION inference model is then the joint feature-point probability and uncertainty networks (bottom middle). The combination of pixel-wise probability and uncertainty forms our definition of featureness F (bottom right), used to describe the general utility of the visual information.
          </p>
          <img style="width:100%;" src="static/images/VISION-network7.png">
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End Method -->

<style>
  .result-caption {
    font-size: 20px;
  }
</style>

<!-- Results -->
<!-- End Results -->



  <section class="section is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{turkar2025VISION,
        title={Visual Inspection System with Intelligent Observation and Navigation with VISION},
        author={Yash Turkar,Yashom Dighe and Karthik Dantu},
        year={2025}
      }
        <!-- booktitle={ECCV}, -->
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
<!-- End of Statcounter Code -->

  </body>
  </html>
